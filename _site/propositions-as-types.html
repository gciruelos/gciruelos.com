<?xml version="1.0" encoding="UTF-8" ?>
<!DOCTYPE html PUBLIC "-//W3C//DTD XHTML 1.0 Strict//EN" "http://www.w3.org/TR/xhtml1/DTD/xhtml1-strict.dtd">
<html xmlns="http://www.w3.org/1999/xhtml" xml:lang="en" lang="en">
    <head>
        <meta http-equiv="Content-Type" content="text/html; charset=UTF-8" />
        <title>gciruelos - Propositions as types</title> 
        <link rel="stylesheet" type="text/css" href="./css/default.css" />
    </head>
    <body>
        <div id="header">
            <div id="logo">
                <a href="./">Gonzalo Ciruelos</a>
            </div> 
            <div id="leblog">
                Blog
            </div>
        </div>
        <div id="content">
            <h1>Propositions as types</h1>
            <div class="info">
    Posted on March  4, 2015
</div>
<p>In Type Theory, <strong>propositions as types</strong> is the idea that types can be interpreted as propositions and vice versa. It is also known as the <strong>Curry-Howard isomorphism</strong> and closely related with the concept of proofs as programs, this is the reason we will use 3 languages during this post: the language of logic, of type theory and Haskell.</p>
<!--more-->
<p>For a basic introduction on type theory, you can read <em>Per Martin-Löf, Intuitionistic Type Theory</em>, or the first chapter of <em>Homotopy Type Theory</em>.</p>
<p>So let’s make the similarities explicit.</p>
<table>
<thead>
<tr class="header">
<th align="center">logic</th>
<th align="center">type theory</th>
<th align="center">Haskell</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td align="center">\(p\)</td>
<td align="center">\(A\)</td>
<td align="center"><code>a</code></td>
</tr>
<tr class="even">
<td align="center">\(p\wedge q\)</td>
<td align="center">\(A \times B\)</td>
<td align="center"><code>(a,b)</code></td>
</tr>
<tr class="odd">
<td align="center">\( p \vee q\)</td>
<td align="center">\(A+B\)</td>
<td align="center"><code>Either a b</code></td>
</tr>
<tr class="even">
<td align="center">\( p\Rightarrow q\)</td>
<td align="center">\( A \to B\)</td>
<td align="center"><code>a -&gt; b</code></td>
</tr>
<tr class="odd">
<td align="center">\(\forall x  P(x) \)</td>
<td align="center">\(\prod_{a:A} P(a)\)</td>
<td align="center"></td>
</tr>
<tr class="even">
<td align="center">\(\exists x P(x) \)</td>
<td align="center">\(\sum_{a:A} P(a)\)</td>
<td align="center"></td>
</tr>
</tbody>
</table>
<p>And the row that conveys the meaning to it all:</p>
<table>
<thead>
<tr class="header">
<th align="center">logic</th>
<th align="center">type theory</th>
<th align="center">Haskell</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td align="center">proof</td>
<td align="center">inhabitant</td>
<td align="center">program</td>
</tr>
</tbody>
</table>
<p>Let’s do an example. Suppose we want to prove the proposition \( p \Rightarrow (q \Rightarrow p)\). Under this interpretation, it suffices to give a Haskell function of signature <code>a -&gt; (b -&gt; a)</code> (which is the same as <code>a -&gt; b -&gt; a</code> because <code>-&gt;</code> is right associative).</p>
<p>You can think a little bit about it, it is really easy:</p>
<pre class="sourceCode haskell"><code class="sourceCode haskell"><span class="ot">proof ::</span> a <span class="ot">-&gt;</span> b <span class="ot">-&gt;</span> a
proof <span class="fu">=</span> \a <span class="ot">-&gt;</span> \b <span class="ot">-&gt;</span> a</code></pre>
<p>Note that the language of type theory and Haskell are very similar. (Haskell isn’t really as powerful as ML type theory, that’s why there are blank spaces on the first table).</p>
<p>Now, you should have noticed that there is something missing on the first table: negation. So let’s introduce it.</p>
<table>
<thead>
<tr class="header">
<th align="center">logic</th>
<th align="center">type theory</th>
<th align="center">Haskell</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td align="center">\(\neg A \)</td>
<td align="center">\(A \to 0\)</td>
<td align="center"><code>a -&gt; Void</code></td>
</tr>
</tbody>
</table>
<p>This arises some questions. First, what is \(0\)? Then, why is that defined like that? And lastly, how is <code>Void</code> defined in Haskell?</p>
<p>The first question is easily answered, \(0\) is the type that has no constructors. So there is no haw to make up an inhabitant of \(0\). Now, why is negation defined like that? Simply because it makes sense. Lastly, how is the Void type defined in haskell? Well, it is de type with no constructors, so</p>
<pre class="sourceCode haskell"><code class="sourceCode haskell"><span class="kw">data</span> <span class="dt">Void</span></code></pre>
<p>As simple as that.</p>
<p>Let’s do now another example. We want to prove \( p \Rightarrow \neg \neg p \). So it suffices to find a haskell function of signature <code>a -&gt; ((a -&gt; Void) -&gt; Void)</code>. Remember that <code>-&gt;</code> is right-associative, so that is the same as <code>a -&gt; (a -&gt; Void) -&gt; Void</code>. Here is a solution,</p>
<pre class="sourceCode haskell"><code class="sourceCode haskell"><span class="ot">proof ::</span> a <span class="ot">-&gt;</span> (a <span class="ot">-&gt;</span> <span class="dt">Void</span>) <span class="ot">-&gt;</span> <span class="dt">Void</span>
proof <span class="fu">=</span> \a <span class="ot">-&gt;</span> \f <span class="ot">-&gt;</span> f a</code></pre>
<p>Note that in the same fashion we can prove <code>a -&gt; (a -&gt; b) -&gt; b</code>. In the language of type theory, that proof would be: \[ \lambda a : A . \lambda f : A \to B .f(b) \ \ :\ \ A \to ((A \to B) \to B) \]</p>
<p>Now let’s try to prove the Law of Excluded Middle (LEM henceforth), i.e. \(p \vee \neg p\). In the language of type thoery, we have to give an inhabitant of \(A + (A \to 0)), but this is impossible in general. If you want to get convinced of that, try to give an element of type <code>Either a (a -&gt; Void)</code> in Haskell. It is impossible, becuase it would be something like <code>Left x</code> where <code>x :: a</code> or <code>Right y</code> where <code>y :: a -&gt; Void</code>.</p>
<p>This gives you some intuition on why Type Theory is not isomorphic with classical logic, but rather intuitionistic, or <strong>constructive logic</strong>. For a detailed proof you can see [1]. But this actually makes sense, because semantics of classical propositional formulas are defined in terms of truth values (true and false), and semantics of intuitionistic formulas (can be) defined in terms of provability.</p>
<h2 id="proof-assistants">Proof assistants</h2>
<p>This is the concept on which proof assistants like <a href="https://coq.inria.fr/">Coq</a> or <a href="http://wiki.portal.chalmers.se/agda/pmwiki.php">Agda</a> are based on. They have fancier type systems than Haskell, but the principles are the same. They implement dependent type theories with inductive types, and that is strong enough to do what they are designed for. [2]</p>
<p>So basically we have to start defining types and proving things about them. To prove a proposition about all elements of a type we often use the induction principle of the type. For example, we can define the naturals as</p>
<p>\[ 0 : \mathbb{N} \] \[ s : \mathbb{N} \to \mathbb{N} \] \[ ind_{\mathbb{N}}(P) : P(0) \to (\prod_{n : \mathbb{N}} P(n) \to P(s(n))) \prod_{n : \mathbb{N}} P(n)\]</p>
<p>This is literally the induction on the naturals. Prove the base case, then prove that vor every natural \(P(n)\) implies \(P(n+1)\) and then you are done. For more on induction, refer to [3].</p>
<h2 id="homotopy-type-theory">Homotopy Type Theory</h2>
<p>Until now, we have talked about intuitionistic type theory, developed by Per Martin-Löf, but this applies more or less to all type theories (like simply typed lambda calculus or calculus of constructions). But it is worth to mention the case of HoTT. HoTT is an intensional type theory [(i.e. makes a difference between definitional equality and the equality type, thus admitting a higher-dimensional interpretation of the latter).] [3]</p>
<p>In HoTT not all types can be interpreted as propositions. This is due to the higher-dimensional interpretation we talked about, as types can contan more information than mere provability. So we would like to restrict types that we can interpret as propositions. This is the definition:</p>
<p>\[isProp(P) :\equiv \prod_{x,y:P} (x=y)\]</p>
<p>What does it mean? It means that \(P\) is a proposition if given two proofs of \(P\), \(x, y\) then they are equal. This means we <em>don’t care</em> about proofs being higher-dimensionally different, they are just proofs.</p>
<p>As you can imagine, sometimes given an arbitrary type \(A\) we would like to work with it as if it was a mere proposition. We can actually do this, defining an operation \(||-||\) such that \(||A||\) is a mere proposition for every type \(A\). This is explained in detail in the section 3.7. of the book.</p>
<h2 id="haskell">Haskell</h2>
<p>Haskell not only lacks dependent types, it has another problem when one tries to do this kind of stuff. Haskell cannot avoid non-terminating programs, so \(\bot\) (bottom) inhabits every type. One can argue this is a feature rather than a bug (and it is), but take a look at this:</p>
<pre class="sourceCode haskell"><code class="sourceCode haskell"><span class="kw">data</span> <span class="dt">Void</span>                  <span class="co">-- has no constructors, thus no inhabitants, right?</span>

<span class="ot">fix ::</span> (a <span class="ot">-&gt;</span> a) <span class="ot">-&gt;</span> a       <span class="co">-- defined in Data.Function</span>
fix f <span class="fu">=</span> <span class="kw">let</span> x <span class="fu">=</span> f x <span class="kw">in</span> x   <span class="co">-- it is the least fixed point of f,</span>
                           <span class="co">-- i.e., the least defined x such that f x = x</span>

<span class="ot">false ::</span> <span class="dt">Void</span>
false <span class="fu">=</span> fix (id<span class="ot"> ::</span> <span class="dt">Void</span> <span class="ot">-&gt;</span> <span class="dt">Void</span>)</code></pre>
<p>And it compiles. So we proved falsity. How? This happens because <code>fix id</code> will always hang, it is a nonterminating function, so for every type we have a</p>
<pre class="sourceCode haskell"><code class="sourceCode haskell"><span class="ot">inhabitant ::</span> a
inhabitant <span class="fu">=</span> fix id</code></pre>
<h2 id="references">References</h2>
<p>[1] Sørenson, Morten; Urzyczyn, Paweł, <em>Lectures on the Curry-Howard Isomorphism</em>. Chapter 4</p>
<p>[2] <a href="https://coq.inria.fr/refman/Reference-Manual006.html">Calculus of Indutive Constructions</a></p>
<p>[3] <em>Homotopy Type Theory</em>. Chapter 1</p>


        </div>
        <div id="footer">
            <a href="https://github.com/gciruelos">github</a> |
            Site proudly generated by
            <a href="http://jaspervdj.be/hakyll">Hakyll</a>
        </div>
    </body>

     
    <script type="text/x-mathjax-config">
         MathJax.Hub.Config({TeX:{extensions: ["AMSmath.js", "AMSsymbols.js"]},
                                  extensions:["tex2jax.js"],
                                  tex2jax: {
                                             inlineMath: [ ["\\(","\\)"] ],
                                             displayMath: [ ["\\[","\\]"] ],
                                             processEscapes: true
                                           },
                                  "HTML-CSS": { availableFonts: ["TeX"] }});
    </script>
    <script type="text/javascript" src="http://cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-AMS_HTML-full"></script>
    <script>
      (function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
      (i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
      m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
      })(window,document,'script','//www.google-analytics.com/analytics.js','ga');
    
      ga('create', 'UA-60347053-1', 'auto');
      ga('send', 'pageview');
    </script>
</html>
